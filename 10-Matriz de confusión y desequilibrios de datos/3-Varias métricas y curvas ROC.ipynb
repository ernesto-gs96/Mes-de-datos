{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: More metrics derived from confusion matrices\n",
        "\n",
        "In this exercise we will learn about different metrics, using them to explain the results obtained from the *binary classification model* we built in the previous unit.\n",
        "\n",
        "## Data visualization\n",
        "\n",
        "We will use the dataset with different classes of objects found on the mountain one more time:\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import numpy\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
        "\n",
        "#Let's have a look at the data\n",
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--2023-03-31 01:56:04--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21511 (21K) [text/plain]\nSaving to: ‘graphing.py.2’\n\ngraphing.py.2       100%[===================>]  21.01K  --.-KB/s    in 0s      \n\n2023-03-31 01:56:04 (118 MB/s) - ‘graphing.py.2’ saved [21511/21511]\n\n--2023-03-31 01:56:06--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 143797 (140K) [text/plain]\nSaving to: ‘snow_objects.csv.2’\n\nsnow_objects.csv.2  100%[===================>] 140.43K  --.-KB/s    in 0.002s  \n\n2023-03-31 01:56:06 (56.8 MB/s) - ‘snow_objects.csv.2’ saved [143797/143797]\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "           size  roughness  color    motion   label\n0     50.959361   1.318226  green  0.054290    tree\n1     60.008521   0.554291  brown  0.000000    tree\n2     20.530772   1.097752  white  1.380464    tree\n3     28.092138   0.966482   grey  0.650528    tree\n4     48.344211   0.799093   grey  0.000000    tree\n...         ...        ...    ...       ...     ...\n2195   1.918175   1.182234  white  0.000000  animal\n2196   1.000694   1.332152  black  4.041097  animal\n2197   2.331485   0.734561  brown  0.961486  animal\n2198   1.786560   0.707935  black  0.000000  animal\n2199   1.518813   1.447957  brown  0.000000  animal\n\n[2200 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>size</th>\n      <th>roughness</th>\n      <th>color</th>\n      <th>motion</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50.959361</td>\n      <td>1.318226</td>\n      <td>green</td>\n      <td>0.054290</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>60.008521</td>\n      <td>0.554291</td>\n      <td>brown</td>\n      <td>0.000000</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20.530772</td>\n      <td>1.097752</td>\n      <td>white</td>\n      <td>1.380464</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.092138</td>\n      <td>0.966482</td>\n      <td>grey</td>\n      <td>0.650528</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48.344211</td>\n      <td>0.799093</td>\n      <td>grey</td>\n      <td>0.000000</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2195</th>\n      <td>1.918175</td>\n      <td>1.182234</td>\n      <td>white</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2196</th>\n      <td>1.000694</td>\n      <td>1.332152</td>\n      <td>black</td>\n      <td>4.041097</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>2.331485</td>\n      <td>0.734561</td>\n      <td>brown</td>\n      <td>0.961486</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>1.786560</td>\n      <td>0.707935</td>\n      <td>black</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2199</th>\n      <td>1.518813</td>\n      <td>1.447957</td>\n      <td>brown</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n  </tbody>\n</table>\n<p>2200 rows × 5 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that to use the dataset above for *binary classification*, we need to add another column to the dataset, and set it to `True` where the original label is `hiker`, and `False` where it's not.\n",
        "\n",
        "Let's then add that label, split the dataset and train the model again:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add a new label with true/false values to our dataset\n",
        "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
        "\n",
        "# define a random forest model\n",
        "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
        "\n",
        "# Define which features are to be used \n",
        "features = [\"size\", \"roughness\", \"motion\"]\n",
        "\n",
        "# Train the model using the binary label\n",
        "model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Model trained!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/py38_default/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model trained!\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use this model to predict whether objects in the snow are hikers or not.\n",
        "\n",
        "Let's plot its *confusion matrix*:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn has a very convenient utility to build confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Calculate the model's accuracy on the TEST set\n",
        "actual = test.is_hiker\n",
        "predictions = model.predict(test[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Create the list of unique labels in the test set, to use in our plot\n",
        "# I.e., ['True', 'False',]\n",
        "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'dask.array' has no attribute 'lib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sklearn has a very convenient utility to build confusion matrices\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure_factory\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mff\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate the model's accuracy on the TEST set\u001b[39;00m\n\u001b[1;32m      6\u001b[0m actual \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mis_hiker\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/figure_factory/__init__.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_imports\u001b[38;5;241m.\u001b[39mget_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure_factory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_county_choropleth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_choropleth\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure_factory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hexbin_mapbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_hexbin_mapbox\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_choropleth\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/figure_factory/_hexbin_mapbox.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_dataframe\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_doc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_docstring\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chart_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m choropleth_mapbox, scatter_mapbox\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/express/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mPlotly express requires pandas to be installed.\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_imshow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imshow\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chart_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     scatter,\n\u001b[1;32m     18\u001b[0m     scatter_3d,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     density_mapbox,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     set_mapbox_access_token,\n\u001b[1;32m     57\u001b[0m     defaults,\n\u001b[1;32m     58\u001b[0m     get_trendline_results,\n\u001b[1;32m     59\u001b[0m     NO_COLOR,\n\u001b[1;32m     60\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/express/_imshow.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_array_to_data_uri\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     xarray_imported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing, tutorial\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     load_dataarray,\n\u001b[1;32m      4\u001b[0m     load_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_mfdataset,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrasterio_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_rasterio\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/testing.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m duck_array_ops, formatting, utils\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataArray\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/core/duck_array_ops.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m take, tensordot, transpose, unravel_index  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m where \u001b[38;5;28;01mas\u001b[39;00m _where\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dask_array_compat, dask_array_ops, dtypes, npcompat, nputils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nanfirst, nanlast\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpycompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_array_type, dask_array_type, is_duck_dask_array\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/core/dask_array_compat.py:60\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m da \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     sliding_window_view \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39msliding_window_view\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     sliding_window_view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'dask.array' has no attribute 'lib'"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also calculate some values that will be used throughout this exercise\n",
        "# We already have actual values and corresponding predictions, defined above\n",
        "correct = actual == predictions\n",
        "tp = numpy.sum(correct & actual)\n",
        "tn = numpy.sum(correct & numpy.logical_not(actual))\n",
        "fp = numpy.sum(numpy.logical_not(correct) & actual)\n",
        "fn = numpy.sum(numpy.logical_not(correct) & numpy.logical_not(actual))\n",
        "\n",
        "print(\"TP - True Positives: \", tp)\n",
        "print(\"TN - True Negatives: \", tn)\n",
        "print(\"FP - False positives: \", fp)\n",
        "print(\"FN - False negatives: \", fn)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the values and matrix above to help us understand other metrics.\n",
        "\n",
        "\n",
        "## Calculating metrics\n",
        "\n",
        "From here on we will take a closer look at each at the following metrics, how they are calculated and how they can help explain our current model. \n",
        "\n",
        "* Accuracy\n",
        "* Sensitivity/Recall\n",
        "* Specificity\n",
        "* Precision\n",
        "* False positive rate\n",
        "\n",
        "Let's first recall some useful terms:\n",
        "\n",
        "* TP = True positives: a positive label is correctly predicted\n",
        "* TN = True negatives: a negative label is correctly predicted\n",
        "* FP = False positives: a negative label is predicted as a positive\n",
        "* FN = False negatives: a positive label is predicted as a negative\n",
        "\n",
        "\n",
        "### Accuracy\n",
        "Accuracy is the number of correct predictions divided by the total number of predictions:\n",
        "\n",
        "```\n",
        "    accuracy = (TP+TN) / number of samples\n",
        "```\n",
        "\n",
        "It's possibly the most basic metric used but, as we've seen, it's not the most reliable when *imbalanced datasets* are used.\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "# len(actual) is the number of samples in the set that generated TP and TN\n",
        "accuracy = (tp+tn) / len(actual) \n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model accuracy is {accuracy:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sensitivity/Recall\n",
        "\n",
        "*Sensitivity* and *Recall* are interchangeable names for the same metric, which expresses the fraction of samples __correctly__ predicted by a model:\n",
        "\n",
        "\n",
        "```\n",
        "    sensitivity = recall = TP / (TP + FN)\n",
        "```\n",
        "\n",
        "This is an important metric, that tells us how out of all the *actually* __positive__ samples, how many are __correctly__ predicted as positive.\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# code for sensitivity/recall\n",
        "sensitivity = recall = tp / (tp + fn)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model sensitivity/recall is {sensitivity:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specificity\n",
        "Specificity expresses the fraction of __negative__ labels correctly predicted over the total number of existing negative samples:\n",
        "\n",
        "```\n",
        "    specificity = TN / (TN + FP)\n",
        "```\n",
        "\n",
        "Tells us how out of all the *actually* __negative__ samples, how many are __correctly__ predicted as negative\n",
        "\n",
        "It can be calculated using the following code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model specificity is {specificity:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precision\n",
        "Precision expresses the proportion of __correctly__ predicted positive samples over all positive predictions:\n",
        "\n",
        "```\n",
        "    precision = TP / (TP + FP)\n",
        "```\n",
        "In other words, it indicates how out of all positive predictions, how many are truly positive labels.\n",
        "\n",
        "It can be calculated using the following code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for precision\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model precision is {precision:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### False positive rate\n",
        "False positive rate or FPR, is the number of __incorrect__ positive predictions divided by the total number of negative samples:\n",
        "\n",
        "```\n",
        "    false_positive_rate = FP / (FP + TN)\n",
        "```\n",
        "\n",
        "Out of all the actual negatives, how many were misclassified as positives?\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for false positive rate\n",
        "false_positive_rate = fp / (fp + tn)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model false positive rate is {false_positive_rate:.2f}%\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the sum of `specificity` and `false positive rate` should always be equal to `1`.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "There are several different metrics that can help us evaluate the performance of a model, in the context of the quality of its predictions.\n",
        "\n",
        "The choice of the most adequate metrics, however, is primarily a funciton of the data and the problem we are trying to solve."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "We covered the following topics in this unit:\n",
        "\n",
        "* How to calculate the very basic measurements used in the evaluation of classification models: TP, FP, TN, FN.\n",
        "* How to use the measurement aboves to calculate more meaningful metrics, such as:\n",
        "    * Accuracy\n",
        "    * Sensitivity/Recall\n",
        "    * Specificity\n",
        "    * Precision\n",
        "    * False positive rate\n",
        "* How the choice of metrics depends on the dataset and the problem we are trying to solve.\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-py38_default-py"
    },
    "kernelspec": {
      "name": "conda-env-py38_default-py",
      "language": "python",
      "display_name": "py38_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}