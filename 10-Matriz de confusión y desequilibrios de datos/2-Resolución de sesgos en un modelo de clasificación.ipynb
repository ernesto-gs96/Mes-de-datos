{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Imbalanced data model bias\n",
        "\n",
        "In this exercise, we will take a closer look at *imbalanced datasets*, what effects they have on predictions and how they can be addressed.\n",
        "\n",
        "We will also employ *confusion matrices* to evaluate model updates.\n",
        "\n",
        "## Data visualization\n",
        "\n",
        "Just like in the previous exercise, we use a dataset that represents different classes of objects found on the mountain:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects_balanced.csv\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
        "\n",
        "# Let's have a look at the data\n",
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--2023-03-31 01:53:59--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21511 (21K) [text/plain]\nSaving to: ‘graphing.py.1’\n\ngraphing.py.1       100%[===================>]  21.01K  --.-KB/s    in 0s      \n\n2023-03-31 01:53:59 (126 MB/s) - ‘graphing.py.1’ saved [21511/21511]\n\n--2023-03-31 01:54:01--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 143797 (140K) [text/plain]\nSaving to: ‘snow_objects.csv.1’\n\nsnow_objects.csv.1  100%[===================>] 140.43K  --.-KB/s    in 0.007s  \n\n2023-03-31 01:54:01 (20.1 MB/s) - ‘snow_objects.csv.1’ saved [143797/143797]\n\n--2023-03-31 01:54:02--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects_balanced.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 131615 (129K) [text/plain]\nSaving to: ‘snow_objects_balanced.csv’\n\nsnow_objects_balanc 100%[===================>] 128.53K  --.-KB/s    in 0.002s  \n\n2023-03-31 01:54:02 (62.7 MB/s) - ‘snow_objects_balanced.csv’ saved [131615/131615]\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "           size  roughness  color    motion   label\n0     50.959361   1.318226  green  0.054290    tree\n1     60.008521   0.554291  brown  0.000000    tree\n2     20.530772   1.097752  white  1.380464    tree\n3     28.092138   0.966482   grey  0.650528    tree\n4     48.344211   0.799093   grey  0.000000    tree\n...         ...        ...    ...       ...     ...\n2195   1.918175   1.182234  white  0.000000  animal\n2196   1.000694   1.332152  black  4.041097  animal\n2197   2.331485   0.734561  brown  0.961486  animal\n2198   1.786560   0.707935  black  0.000000  animal\n2199   1.518813   1.447957  brown  0.000000  animal\n\n[2200 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>size</th>\n      <th>roughness</th>\n      <th>color</th>\n      <th>motion</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50.959361</td>\n      <td>1.318226</td>\n      <td>green</td>\n      <td>0.054290</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>60.008521</td>\n      <td>0.554291</td>\n      <td>brown</td>\n      <td>0.000000</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20.530772</td>\n      <td>1.097752</td>\n      <td>white</td>\n      <td>1.380464</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28.092138</td>\n      <td>0.966482</td>\n      <td>grey</td>\n      <td>0.650528</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48.344211</td>\n      <td>0.799093</td>\n      <td>grey</td>\n      <td>0.000000</td>\n      <td>tree</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2195</th>\n      <td>1.918175</td>\n      <td>1.182234</td>\n      <td>white</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2196</th>\n      <td>1.000694</td>\n      <td>1.332152</td>\n      <td>black</td>\n      <td>4.041097</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>2.331485</td>\n      <td>0.734561</td>\n      <td>brown</td>\n      <td>0.961486</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2198</th>\n      <td>1.786560</td>\n      <td>0.707935</td>\n      <td>black</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n    <tr>\n      <th>2199</th>\n      <td>1.518813</td>\n      <td>1.447957</td>\n      <td>brown</td>\n      <td>0.000000</td>\n      <td>animal</td>\n    </tr>\n  </tbody>\n</table>\n<p>2200 rows × 5 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we have an *imbalanced dataset*. Some classes are much more frequent than others:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import graphing # custom graphing code. See our GitHub repo for details\n",
        "\n",
        "# Plot a histogram with counts for each label\n",
        "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/py38_default/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'dask.array' has no attribute 'lib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphing\u001b[39;00m \u001b[38;5;66;03m# custom graphing code. See our GitHub repo for details\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot a histogram with counts for each label\u001b[39;00m\n\u001b[1;32m      4\u001b[0m graphing\u001b[38;5;241m.\u001b[39mmultiple_histogram(dataset, label_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, label_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/learn/graphing.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfromnumeric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m repeat, shape\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgraph_objects\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/express/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mPlotly express requires pandas to be installed.\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_imshow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imshow\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chart_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     scatter,\n\u001b[1;32m     18\u001b[0m     scatter_3d,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     density_mapbox,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     set_mapbox_access_token,\n\u001b[1;32m     57\u001b[0m     defaults,\n\u001b[1;32m     58\u001b[0m     get_trendline_results,\n\u001b[1;32m     59\u001b[0m     NO_COLOR,\n\u001b[1;32m     60\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/plotly/express/_imshow.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_array_to_data_uri\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     xarray_imported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing, tutorial\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     load_dataarray,\n\u001b[1;32m      4\u001b[0m     load_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_mfdataset,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrasterio_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_rasterio\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/testing.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m duck_array_ops, formatting, utils\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataArray\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/core/duck_array_ops.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m take, tensordot, transpose, unravel_index  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m where \u001b[38;5;28;01mas\u001b[39;00m _where\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dask_array_compat, dask_array_ops, dtypes, npcompat, nputils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nanfirst, nanlast\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpycompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_array_type, dask_array_type, is_duck_dask_array\n",
            "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/xarray/core/dask_array_compat.py:60\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m da \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     sliding_window_view \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39msliding_window_view\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     sliding_window_view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'dask.array' has no attribute 'lib'"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using binary classification\n",
        "\n",
        "For this exercise we will build a *binary classification model*. We want to predict if objects in the snow are \"hikers\" or \"not-hikers\".\n",
        "\n",
        "To do that, we first need to add another column to our dataset, and set it to `True` where the original label is `hiker`, and `False` to anything else:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new label with true/false values to our dataset\n",
        "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
        "\n",
        "# Plot frequency for new label\n",
        "graphing.multiple_histogram(dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Distribution for new binary label 'is_hiker'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have only two classes of labels in our dataset, but we have made it even more imbalanced.\n",
        "\n",
        "Let's train the random forest model using `is_hiker` as the target variable, then measure its accuracy on both *train* and *test* sets:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Custom function that measures accuracy on different models and datasets\n",
        "# We will use this in different parts of the exercise\n",
        "def assess_accuracy(model, dataset, label):\n",
        "    \"\"\"\n",
        "    Asesses model accuracy on different sets\n",
        "    \"\"\" \n",
        "    actual = dataset[label]        \n",
        "    predictions = model.predict(dataset[features])\n",
        "    acc = accuracy_score(actual, predictions)\n",
        "    return acc\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
        "\n",
        "# define a random forest model\n",
        "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
        "\n",
        "# Define which features are to be used (leave color out for now)\n",
        "features = [\"size\", \"roughness\", \"motion\"]\n",
        "\n",
        "# Train the model using the binary label\n",
        "model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Train accuracy:\", assess_accuracy(model,train, \"is_hiker\"))\n",
        "print(\"Test accuracy:\", assess_accuracy(model,test, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy looks good for both *train* and *test* sets, but remember that this metric is not an absolute measure of success.\n",
        "\n",
        "We should plot a confusion matrix to see how the model is actually doing:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn has a very convenient utility to build confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Calculate the model's accuracy on the TEST set\n",
        "actual = test.is_hiker\n",
        "predictions = model.predict(test[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Create the list of unique labels in the test set, to use in our plot\n",
        "# I.e., ['True', 'False',]\n",
        "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows us that, despite the reported metrics, the model is not incredibly precise.\n",
        "\n",
        "Out of the 660 samples present in the *test* set (30% of the total samples), it predicted `29` *false negatives* and `33` _false positives_.\n",
        "\n",
        "More importantly, look at the bottom row, which shows what happened when the model was shown information about a hiker: it got the answer wrong almost 30% of the time. This means it would not correctly identify almost 30% of the people on the mountain!  \n",
        "\n",
        "What happens if we used this model to make predictions on balanced sets?\n",
        "\n",
        "Let's load a dataset with an equal number of outcomes for \"hikers\" and \"non-hikers\", then use that data to make predictions:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and print umbiased set\n",
        "#Import the data from the .csv file\n",
        "balanced_dataset = pandas.read_csv('snow_objects_balanced.csv', delimiter=\"\\t\")\n",
        "\n",
        "#Let's have a look at the data\n",
        "graphing.multiple_histogram(balanced_dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This new dataset is balanced among the classes, but for our purposes we want it balanced between hikers and non-hikers. \n",
        "\n",
        "For simplicity, let's take the hikers plus a random sampling of the non-hikers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add a new label with true/false values to our dataset\n",
        "balanced_dataset[\"is_hiker\"] = balanced_dataset.label == \"hiker\"\n",
        "\n",
        "hikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == 1] \n",
        "nonhikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == False] \n",
        "# take a random sampling of non-hikers the same size as the hikers subset\n",
        "nonhikers_dataset = nonhikers_dataset.sample(n=len(hikers_dataset.index), random_state=1)\n",
        "balanced_dataset = pandas.concat([hikers_dataset, nonhikers_dataset])\n",
        "\n",
        "# Plot frequency for \"is_hiker\" labels\n",
        "graphing.multiple_histogram(balanced_dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Label distribution in balanced dataset\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the `is_hiker` label has the same number of `True` and `False` for both classes. We are now using a *class balanced dataset*.\n",
        "\n",
        "Let's run predictions on this set using the previously trained model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using a balanced dataset\n",
        "actual = balanced_dataset.is_hiker\n",
        "predictions = model.predict(balanced_dataset[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Print accuracy using this set\n",
        "print(\"Balanced set accuracy:\", assess_accuracy(model,balanced_dataset, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, we see a noticeable drop in accuracy using a different set.\n",
        "\n",
        "Again, let's visually analyze its performance:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# plot new confusion matrix\n",
        "# Create the list of unique labels in the test set to use in our plot\n",
        "unique_targets = sorted(list(balanced_dataset[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix confirms the poor accuracy using this dataset, but why is this happening when we had such excellent metrics in the earlier *train* and *test* sets?\n",
        "\n",
        "Recall that the first model was heavily imbalanced. The \"hiker\" class made up roughly 22% of the outcomes.\n",
        "\n",
        "When such an imbalance happens, classification models don't have enough data to learn the patterns for the minority __class__, and as a consequence become biased towards the __majority__ class!\n",
        "\n",
        "Imbalanced sets can be addressed in a number of ways:\n",
        "\n",
        "- Improving data selection\n",
        "- Resampling the dataset\n",
        "- Using weighted classes\n",
        "\n",
        "For this exercise, we will focus on the last option."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using class weights to balance dataset\n",
        "\n",
        "We can assign different *weights* to the majority and minority classes, according to their distribution, and modify our training algorithm so that it takes that information into account during the training phase.\n",
        "\n",
        "It will then penalize errors when the minority class is misclassified, in essence \"forcing\" the model to to better learn their features and patterns.\n",
        "\n",
        "To use weighted classes, we have to retrain our model using the original *train* set, but this time telling the algorithm to use weights when calculating errors:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function used in calculating weights\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Retrain model using class weights\n",
        "# Using class_weight=\"balanced\" tells the algorithm to automatically calculate weights for us\n",
        "weighted_model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False, class_weight=\"balanced\")\n",
        "# Train the weighted_model using binary label\n",
        "weighted_model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Train accuracy:\", assess_accuracy(weighted_model,train, \"is_hiker\"))\n",
        "print(\"Test accuracy:\", assess_accuracy(weighted_model, test, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using the weighted classes, the *train* accuracy remained almost the same, while the *test* accuracy showed a small improvement (roughly 1%).\n",
        "\n",
        "Let's see if results are improved at all using the __balanced__ set for predictions again:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Balanced set accuracy:\", assess_accuracy(weighted_model, balanced_dataset, \"is_hiker\"))\n",
        "\n",
        "# Test the weighted_model using a balanced dataset\n",
        "actual = balanced_dataset.is_hiker\n",
        "predictions = weighted_model.predict(balanced_dataset[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for the balanced set increased roughly 4%, but we should still try to to visualize and understand the new results.\n",
        "\n",
        "## Final confusion matrix\n",
        "\n",
        "We can now plot a final confusion matrix, representing predictions for a *balanced dataset*, using a model trained on a *weighted class dataset*:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the results might look a bit disappointing, we now have 21% wrong predictions (FNs + FPs), against 25% from the previous experiment.\n",
        "\n",
        "Correct predictions (TPs + TNs) went from 74.7% to 78.7%.\n",
        "\n",
        "Is an all around 4% improvement significant or not?\n",
        "\n",
        "Remember that we had relatively little data to train the model, and the features we have available may still be so similar for different samples (for example, hikers and animals tend to be small, non-rough and move a lot), that despite our efforts, the model still has some difficulty making correct predictions.\n",
        "\n",
        "We only had to change a single line of code to get better results, so it seems worth the effort!\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This was a long exercise, where we covered the following topics:\n",
        "\n",
        "- Creating new label fields so we can perform *binary classification* using a dataset with multiple classes.\n",
        "- How training on *imbalanced sets* can have a negative effect in perfomance, especially when using unseen data from *balanced datasets*.\n",
        "- Evaluating results of *binary classification* models using a confusion matrix.\n",
        "- Using weighted classes to address class imbalances when training a model and evaluating the results.\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py38_default-py",
      "language": "python",
      "display_name": "py38_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py38_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}