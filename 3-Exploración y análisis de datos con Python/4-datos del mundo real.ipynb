{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploración de datos con Python - datos del mundo real\n",
        "\n",
        "En el último cuaderno, observamos las calificaciones de los datos de nuestros estudiantes e investigamos los datos visualmente con histogramas y diagramas de caja. Ahora analizaremos casos más complejos, describiremos los datos de manera más completa y discutiremos cómo hacer comparaciones básicas entre los datos.\n",
        "\n",
        "### Distribuciones de datos en el mundo real\n",
        "\n",
        "Anteriormente, analizamos las calificaciones para los datos de nuestros estudiantes y estimamos a partir de esta muestra cómo podría ser la población completa de calificaciones. Vamos a refrescar nuestra memoria y echar un vistazo a estos datos de nuevo.\n",
        "\n",
        "Ejecute el siguiente código para imprimir los datos y hacer un histograma + diagrama de caja que muestre las calificaciones de nuestra muestra de estudiantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-30 00:07:17--  https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/grades.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322 [text/plain]\n",
            "grades.csv: Permission denied\n",
            "\n",
            "Cannot write to ‘grades.csv’ (Success).\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'grades.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load data from a text file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mwget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/grades.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m df_students \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mgrades.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m,header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minfer\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Remove any rows with missing data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df_students \u001b[39m=\u001b[39m df_students\u001b[39m.\u001b[39mdropna(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39many\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'grades.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load data from a text file\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/ml-basics/grades.csv\n",
        "df_students = pd.read_csv('grades.csv',delimiter=',',header='infer')\n",
        "\n",
        "# Remove any rows with missing data\n",
        "df_students = df_students.dropna(axis=0, how='any')\n",
        "\n",
        "# Calculate who passed, assuming '60' is the grade needed to pass\n",
        "passes  = pd.Series(df_students['Grade'] >= 60)\n",
        "\n",
        "# Save who passed to the Pandas dataframe\n",
        "df_students = pd.concat([df_students, passes.rename(\"Pass\")], axis=1)\n",
        "\n",
        "\n",
        "# Print the result out into this notebook\n",
        "print(df_students)\n",
        "\n",
        "\n",
        "# Create a function that we can re-use\n",
        "def show_distribution(var_data):\n",
        "    '''\n",
        "    This function will make a distribution (graph) and display it\n",
        "    '''\n",
        "\n",
        "    # Get statistics\n",
        "    min_val = var_data.min()\n",
        "    max_val = var_data.max()\n",
        "    mean_val = var_data.mean()\n",
        "    med_val = var_data.median()\n",
        "    mod_val = var_data.mode()[0]\n",
        "\n",
        "    print('Minimum:{:.2f}\\nMean:{:.2f}\\nMedian:{:.2f}\\nMode:{:.2f}\\nMaximum:{:.2f}\\n'.format(min_val,\n",
        "                                                                                            mean_val,\n",
        "                                                                                            med_val,\n",
        "                                                                                            mod_val,\n",
        "                                                                                            max_val))\n",
        "\n",
        "    # Create a figure for 2 subplots (2 rows, 1 column)\n",
        "    fig, ax = plt.subplots(2, 1, figsize = (10,4))\n",
        "\n",
        "    # Plot the histogram   \n",
        "    ax[0].hist(var_data)\n",
        "    ax[0].set_ylabel('Frequency')\n",
        "\n",
        "    # Add lines for the mean, median, and mode\n",
        "    ax[0].axvline(x=min_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
        "    ax[0].axvline(x=mean_val, color = 'cyan', linestyle='dashed', linewidth = 2)\n",
        "    ax[0].axvline(x=med_val, color = 'red', linestyle='dashed', linewidth = 2)\n",
        "    ax[0].axvline(x=mod_val, color = 'yellow', linestyle='dashed', linewidth = 2)\n",
        "    ax[0].axvline(x=max_val, color = 'gray', linestyle='dashed', linewidth = 2)\n",
        "\n",
        "    # Plot the boxplot   \n",
        "    ax[1].boxplot(var_data, vert=False)\n",
        "    ax[1].set_xlabel('Value')\n",
        "\n",
        "    # Add a title to the Figure\n",
        "    fig.suptitle('Data Distribution')\n",
        "\n",
        "    # Show the figure\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "show_distribution(df_students['Grade'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como recordarán, nuestros datos tenían la media y el modo en el centro, con datos distribuidos simétricamente desde allí.\n",
        "\n",
        "Ahora echemos un vistazo a la distribución de los datos de horas de estudio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get the variable to examine\n",
        "col = df_students['StudyHours']\n",
        "# Call the function\n",
        "show_distribution(col)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La distribución de los datos del tiempo de estudio es significativamente diferente de la de las calificaciones.\n",
        "\n",
        "Tenga en cuenta que los bigotes del diagrama de caja solo comienzan alrededor de 6.0, lo que indica que la gran mayoría del primer trimestre de los datos está por encima de este valor. El mínimo está marcado con una o, lo que indica que estadísticamente es un valor atípico, un valor que se encuentra significativamente fuera del rango del resto de la distribución.\n",
        "\n",
        "Los valores atípicos pueden ocurrir por muchas razones. Tal vez un estudiante tenía la intención de registrar \"10\" horas de tiempo de estudio, pero ingresó \"1\" y perdió el \"0\". ¡O tal vez el estudiante era anormalmente perezoso cuando se trata de estudiar! De cualquier manera, es una anomalía estadística que no representa a un estudiante típico. Veamos cómo se ve la distribución sin él."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get the variable to examine\n",
        "# We will only get students who have studied more than one hour\n",
        "col = df_students[df_students.StudyHours>1]['StudyHours']\n",
        "\n",
        "# Call the function\n",
        "show_distribution(col)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para fines de aprendizaje, acabamos de tratar el valor 1 como un verdadero valor atípico aquí y lo hemos excluido. En el mundo real, sería inusual excluir datos en los extremos sin más justificación cuando el tamaño de nuestra muestra es tan pequeño. Esto se debe a que cuanto menor sea el tamaño de nuestra muestra, más probable es que nuestro muestreo sea una mala representación de toda la población. (Aquí, la población significa calificaciones para todos los estudiantes, no solo para nuestros 22). Por ejemplo, si tomamos muestras del tiempo de estudio de otros 1,000 estudiantes, ¡podríamos encontrar que en realidad es bastante común no estudiar mucho!\n",
        "\n",
        "Cuando tenemos más datos disponibles, nuestra muestra se vuelve más confiable. Esto hace que sea más fácil considerar los valores atípicos como valores que caen por debajo o por encima de los percentiles dentro de los cuales se encuentran la mayoría de los datos. Por ejemplo, el siguiente código utiliza la función cuantil de Pandas para excluir observaciones por debajo del percentil 0,01 (el valor por encima del cual reside el 99% de los datos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the 0.01th percentile\n",
        "q01 = df_students.StudyHours.quantile(0.01)\n",
        "# Get the variable to examine\n",
        "col = df_students[df_students.StudyHours>q01]['StudyHours']\n",
        "# Call the function\n",
        "show_distribution(col)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Consejo**: También puede eliminar valores atípicos en el extremo superior de la distribución definiendo un umbral con un valor de percentil alto. Por ejemplo, podría usar la función cuantil para encontrar el percentil 0.99 por debajo del cual reside el 99% de los datos.\n",
        "\n",
        "Con los valores atípicos eliminados, el diagrama de caja muestra todos los datos dentro de los cuatro cuartiles. Tenga en cuenta que la distribución no es simétrica como lo es para los datos de grado. Hay algunos estudiantes con tiempos de estudio muy altos de alrededor de 16 horas, pero la mayor parte de los datos es entre 7 y 13 horas. Los pocos valores extremadamente altos tiran de la media hacia el extremo superior de la escala.\n",
        "\n",
        "Veamos la densidad de esta distribución."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_density(var_data):\n",
        "    fig = plt.figure(figsize=(10,4))\n",
        "\n",
        "    # Plot density\n",
        "    var_data.plot.density()\n",
        "\n",
        "    # Add titles and labels\n",
        "    plt.title('Data Density')\n",
        "\n",
        "    # Show the mean, median, and mode\n",
        "    plt.axvline(x=var_data.mean(), color = 'cyan', linestyle='dashed', linewidth = 2)\n",
        "    plt.axvline(x=var_data.median(), color = 'red', linestyle='dashed', linewidth = 2)\n",
        "    plt.axvline(x=var_data.mode()[0], color = 'yellow', linestyle='dashed', linewidth = 2)\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()\n",
        "\n",
        "# Get the density of StudyHours\n",
        "show_density(col)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este tipo de distribución se llama sesgada a la derecha. La masa de los datos está en el lado izquierdo de la distribución, creando una larga cola hacia la derecha debido a los valores en el extremo superior, que tiran de la media hacia la derecha.\n",
        "\n",
        "#### Medidas de varianza\n",
        "\n",
        "Así que ahora tenemos una buena idea de dónde están las distribuciones de datos de la mitad del grado y las horas de estudio. Sin embargo, hay otro aspecto de las distribuciones que debemos examinar: ¿cuánta variabilidad hay en los datos?\n",
        "\n",
        "Las estadísticas típicas que miden la variabilidad en los datos incluyen:\n",
        "\n",
        "- **Rango**: La diferencia entre el máximo y el mínimo. No hay una función incorporada para esto, pero es fácil de calcular usando las funciones **min** y **max**.\n",
        "- **Varianza**: El promedio de la diferencia al cuadrado de la media. Puede usar la función **var** incorporada para encontrar esto.\n",
        "- **Desviación estándar**: La raíz cuadrada de la varianza. Puede usar la función **std** incorporada para encontrar esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for col_name in ['Grade','StudyHours']:\n",
        "    col = df_students[col_name]\n",
        "    rng = col.max() - col.min()\n",
        "    var = col.var()\n",
        "    std = col.std()\n",
        "    print('\\n{}:\\n - Range: {:.2f}\\n - Variance: {:.2f}\\n - Std.Dev: {:.2f}'.format(col_name, rng, var, std))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De estas estadísticas, la desviación estándar es generalmente la más útil. Proporciona una medida de varianza en los datos en la misma escala que los datos en sí (por lo tanto, puntos de calificación para la distribución de calificaciones y horas para la distribución de horas de estudio). Cuanto mayor es la desviación estándar, más varianza hay cuando se comparan los valores de la distribución con la media de la distribución; en otras palabras, los datos están más dispersos.\n",
        "\n",
        "Cuando se trabaja con una distribución normal, la desviación estándar funciona con las características particulares de una distribución normal para proporcionar una visión aún mayor. Ejecute la celda siguiente para ver la relación entre las desviaciones estándar y los datos en la distribución normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Get the Grade column\n",
        "col = df_students['Grade']\n",
        "\n",
        "# get the density\n",
        "density = stats.gaussian_kde(col)\n",
        "\n",
        "# Plot the density\n",
        "col.plot.density()\n",
        "\n",
        "# Get the mean and standard deviation\n",
        "s = col.std()\n",
        "m = col.mean()\n",
        "\n",
        "# Annotate 1 stdev\n",
        "x1 = [m-s, m+s]\n",
        "y1 = density(x1)\n",
        "plt.plot(x1,y1, color='magenta')\n",
        "plt.annotate('1 std (68.26%)', (x1[1],y1[1]))\n",
        "\n",
        "# Annotate 2 stdevs\n",
        "x2 = [m-(s*2), m+(s*2)]\n",
        "y2 = density(x2)\n",
        "plt.plot(x2,y2, color='green')\n",
        "plt.annotate('2 std (95.45%)', (x2[1],y2[1]))\n",
        "\n",
        "# Annotate 3 stdevs\n",
        "x3 = [m-(s*3), m+(s*3)]\n",
        "y3 = density(x3)\n",
        "plt.plot(x3,y3, color='orange')\n",
        "plt.annotate('3 std (99.73%)', (x3[1],y3[1]))\n",
        "\n",
        "# Show the location of the mean\n",
        "plt.axvline(col.mean(), color='cyan', linestyle='dashed', linewidth=1)\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las líneas horizontales muestran el porcentaje de datos dentro de 1, 2 y 3 desviaciones estándar de la media (más o menos).\n",
        "\n",
        "En cualquier distribución normal:\n",
        "- Aproximadamente el 68,26% de los valores caen dentro de una desviación estándar de la media.\n",
        "- Aproximadamente el 95,45% de los valores caen dentro de dos desviaciones estándar de la media.\n",
        "- Aproximadamente el 99,73% de los valores se encuentran dentro de tres desviaciones estándar de la media.\n",
        "\n",
        "Entonces, como sabemos que la nota media es de 49,18, la desviación estándar es de 21,74 y la distribución de las notas es aproximadamente normal, podemos calcular que el 68,26% de los estudiantes deberían alcanzar una nota entre 27,44 y 70,92.\n",
        "\n",
        "La estadística descriptiva que hemos utilizado para entender la distribución de las variables de datos de los estudiantes son la base del análisis estadístico. Debido a que son una parte tan importante de la exploración de los datos, hay un método integrado del objeto DataFrame que devuelve las estadísticas descriptivas principales para todas las columnas numéricas.describe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_students.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparación de datos\n",
        "\n",
        "Ahora que sabe algo sobre la distribución estadística de los datos en su conjunto de datos, está listo para examinar sus datos para identificar cualquier relación aparente entre variables.\n",
        "\n",
        "En primer lugar, vamos a deshacernos de cualquier fila que contenga valores atípicos para que tengamos una muestra que sea representativa de una clase típica de estudiantes. Identificamos que la columna StudyHours contiene algunos valores atípicos con valores extremadamente bajos, por lo que eliminaremos esas filas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sample = df_students[df_students['StudyHours']>1]\n",
        "df_sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparación de variables numéricas y categóricas\n",
        "\n",
        "Los datos incluyen dos variables numéricas (**StudyHours** y **Grade**) y dos variables categóricas (**Name** y **Pass**). Comencemos comparando la columna numérica **StudyHours** con la columna categórica **Pass** para ver si existe una relación aparente entre el número de horas estudiadas y una calificación aprobatoria.\n",
        "\n",
        "Para hacer esta comparación, vamos a crear diagramas de caja que muestren la distribución de StudyHours para cada posible valor de Pass (verdadero y falso).\n",
        "\n",
        "To make this comparison, let's create box plots showing the distribution of StudyHours for each possible Pass value (true and false)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_sample.boxplot(column='StudyHours', by='Pass', figsize=(8,5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparando las distribuciones de StudyHours, es inmediatamente evidente (si no particularmente sorprendente) que los estudiantes que aprobaron el curso tendían a estudiar más horas que los estudiantes que no lo hicieron. Entonces, si desea predecir si es probable que un estudiante apruebe el curso, la cantidad de tiempo que pasa estudiando puede ser un buen indicador predictivo.\n",
        "\n",
        "### Comparación de variables numéricas\n",
        "\n",
        "Ahora comparemos dos variables numéricas. Comenzaremos creando un gráfico de barras que muestre tanto las calificaciones como las horas de estudio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a bar plot of name vs grade and study hours\n",
        "df_sample.plot(x='Name', y=['Grade','StudyHours'], kind='bar', figsize=(8,5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El gráfico muestra barras para las horas de grado y estudio para cada estudiante, pero no es fácil de comparar porque los valores están en diferentes escalas. Una calificación se mide en puntos de calificación (y varía de 3 a 97), y el tiempo de estudio se mide en horas (y varía de 1 a 16).\n",
        "\n",
        "Una técnica común cuando se trata de datos numéricos en diferentes escalas es normalizar los datos para que los valores conserven su distribución proporcional pero se midan en la misma escala. Para lograr esto, usaremos una técnica llamada escala MinMax que distribuye los valores proporcionalmente en una escala de 0 a 1. Puede escribir el código para aplicar esta transformación, pero la biblioteca Scikit-Learn proporciona un escalador para hacerlo por usted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Get a scaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Create a new dataframe for the scaled values\n",
        "df_normalized = df_sample[['Name', 'Grade', 'StudyHours']].copy()\n",
        "\n",
        "# Normalize the numeric columns\n",
        "df_normalized[['Grade','StudyHours']] = scaler.fit_transform(df_normalized[['Grade','StudyHours']])\n",
        "\n",
        "# Plot the normalized values\n",
        "df_normalized.plot(x='Name', y=['Grade','StudyHours'], kind='bar', figsize=(8,5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con los datos normalizados, es más fácil ver una relación aparente entre el grado y el tiempo de estudio. No es una coincidencia exacta, pero definitivamente parece que los estudiantes con calificaciones más altas tienden a haber estudiado más.\n",
        "\n",
        "Así que parece haber una correlación entre el tiempo de estudio y el grado. De hecho, hay una medida de **correlación** estadística que podemos usar para cuantificar la relación entre estas columnas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_normalized.Grade.corr(df_normalized.StudyHours)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La estadística de correlación es un valor entre -1 y 1 que indica la fuerza de una relación. Los valores superiores a 0 indican una correlación positiva (los valores altos de una variable tienden a coincidir con los valores altos de la otra), mientras que los valores inferiores a 0 indican una correlación negativa (los valores altos de una variable tienden a coincidir con valores bajos de la otra). En este caso, el valor de correlación es cercano a 1, mostrando una correlación fuertemente positiva entre el tiempo de estudio y el grado.\n",
        "\n",
        "> **Nota**: Los científicos de datos a menudo citan la máxima \"correlación no es causalidad\". En otras palabras, por muy tentador que sea, no debe interpretar la correlación estadística como una explicación de por qué uno de los valores es alto. En el caso de los datos de los estudiantes, las estadísticas demuestran que los estudiantes con calificaciones altas tienden a tener también altas cantidades de tiempo de estudio, pero esto no es lo mismo que demostrar que lograron altas calificaciones porque estudiaron mucho. La estadística también podría usarse como evidencia para apoyar la conclusión sin sentido de que los estudiantes estudiaron mucho porque sus calificaciones iban a ser altas.\n",
        "\n",
        "Otra forma de visualizar la correlación aparente entre dos columnas numéricas es usar un diagrama de dispersión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a scatter plot\n",
        "df_sample.plot.scatter(title='Study Time vs Grade', x='StudyHours', y='Grade')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez más, parece que hay un patrón discernible en el que los estudiantes que estudiaron la mayor cantidad de horas son también los estudiantes que obtuvieron las calificaciones más altas.\n",
        "\n",
        "Podemos ver esto más claramente agregando una línea de regresión (o una línea de mejor ajuste) al gráfico que muestra la tendencia general en los datos. Para ello, utilizaremos una técnica estadística llamada regresión de mínimos cuadrados.\n",
        "\n",
        "Recuerda cuando estabas aprendiendo a resolver ecuaciones lineales en la escuela, y recuerda que la forma de pendiente-intersección de una ecuación lineal se ve así:\n",
        "\n",
        "$ y = mx + b $\n",
        "\n",
        "En esta ecuación, y y x son las variables de coordenadas, m es la pendiente de la línea y b es la intersección y (donde la línea pasa por el eje Y).\n",
        "\n",
        "En el caso de nuestro diagrama de dispersión para nuestros datos de estudiantes, ya tenemos nuestros valores para x (StudyHours) e y (Grade), por lo que solo necesitamos calcular la intersección y la pendiente de la línea recta que se encuentra más cerca de esos puntos. Entonces podemos formar una ecuación lineal que calcula un nuevo valor y en esa línea para cada uno de nuestros valores x (StudyHours). Para evitar confusiones, llamaremos a este nuevo valor y f(x) (porque es la salida de una ecuación lineal función basada en x). La diferencia entre el valor original y (Nota) y el valor f(x) es el error entre nuestra línea de regresión y la Nota real alcanzada por el estudiante. Nuestro objetivo es calcular la pendiente e interceptar una línea con el error general más bajo.\n",
        "\n",
        "Específicamente, definimos el error general tomando el error para cada punto, cuadrándolo y sumando todos los errores al cuadrado. La línea de mejor ajuste es la línea que nos da el valor más bajo para la suma de los errores al cuadrado, de ahí el nombre de regresión de mínimos cuadrados.\n",
        "\n",
        "Afortunadamente, no es necesario codificar el cálculo de regresión usted mismo. El paquete SciPy incluye una clase stats que proporciona un método linregress para hacer el trabajo duro por usted. Esto devuelve (entre otras cosas) los coeficientes que necesita para la ecuación de pendiente: pendiente (m) e intersección (b) en función de un par dado de muestras variables que desea comparar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "#\n",
        "df_regression = df_sample[['Grade', 'StudyHours']].copy()\n",
        "\n",
        "# Get the regression slope and intercept\n",
        "m, b, r, p, se = stats.linregress(df_regression['StudyHours'], df_regression['Grade'])\n",
        "print('slope: {:.4f}\\ny-intercept: {:.4f}'.format(m,b))\n",
        "print('so...\\n f(x) = {:.4f}x + {:.4f}'.format(m,b))\n",
        "\n",
        "# Use the function (mx + b) to calculate f(x) for each x (StudyHours) value\n",
        "df_regression['fx'] = (m * df_regression['StudyHours']) + b\n",
        "\n",
        "# Calculate the error between f(x) and the actual y (Grade) value\n",
        "df_regression['error'] = df_regression['fx'] - df_regression['Grade']\n",
        "\n",
        "# Create a scatter plot of Grade vs StudyHours\n",
        "df_regression.plot.scatter(x='StudyHours', y='Grade')\n",
        "\n",
        "# Plot the regression line\n",
        "plt.plot(df_regression['StudyHours'],df_regression['fx'], color='cyan')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenga en cuenta que esta vez, el código trazó dos cosas distintas: el gráfico de dispersión de las horas y calificaciones de estudio de muestra se traza como antes, y luego se traza una línea de mejor ajuste basada en los coeficientes de regresión de mínimos cuadrados.\n",
        "\n",
        "Los coeficientes de pendiente e intersección calculados para la línea de regresión se muestran encima de la gráfica.\n",
        "\n",
        "La línea se basa en los valores f(x) calculados para cada valor de StudyHours. Ejecute la celda siguiente para ver una tabla que incluya los siguientes valores:\n",
        "\n",
        "- Las Horas de Estudio para cada estudiante.\n",
        "- La calificación alcanzada por cada estudiante.\n",
        "- El valor f(x) calculado utilizando los coeficientes de línea de regresión.\n",
        "- Error entre el valor f(x) calculado y el valor real de Grade.\n",
        "\n",
        "Algunos de los errores, particularmente en los extremos extremos, y bastante grandes (hasta más de 17.5 puntos de calificación). Pero, en general, la línea está bastante cerca de las calificaciones reales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the original x,y values, the f(x) value, and the error\n",
        "df_regression[['StudyHours', 'Grade', 'fx', 'error']]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uso de los coeficientes de regresión para la predicción\n",
        "\n",
        "Ahora que tiene los coeficientes de regresión para la relación entre el tiempo de estudio y la calificación, puede usarlos en una función para estimar la calificación esperada para una cantidad determinada de estudio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a function based on our regression coefficients\n",
        "def f(x):\n",
        "    m = 6.3134\n",
        "    b = -17.9164\n",
        "    return m*x + b\n",
        "\n",
        "study_time = 14\n",
        "\n",
        "# Get f(x) for study time\n",
        "prediction = f(study_time)\n",
        "\n",
        "# Grade can't be less than 0 or more than 100\n",
        "expected_grade = max(0,min(100,prediction))\n",
        "\n",
        "#Print the estimated grade\n",
        "print ('Studying for {} hours per week may result in a grade of {:.0f}'.format(study_time, expected_grade))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por lo tanto, al aplicar estadísticas a los datos de muestra, ha determinado una relación entre el tiempo de estudio y la calificación y ha encapsulado esa relación en una función general que se puede usar para predecir una calificación para una cantidad determinada de tiempo de estudio.\n",
        "\n",
        "Esta técnica es, de hecho, la premisa básica del aprendizaje automático. Puede tomar un conjunto de datos de ejemplo que incluya una o más características (en este caso, el número de horas estudiadas) y un valor de etiqueta conocido (en este caso, la calificación obtenida) y utilizar los datos de muestra para derivar una función que calcule los valores de etiqueta previstos para cualquier conjunto determinado de características.\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "conda-env-azureml_py38-py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
